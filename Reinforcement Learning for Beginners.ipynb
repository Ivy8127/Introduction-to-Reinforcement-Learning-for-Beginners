{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86f8e5ab",
   "metadata": {},
   "source": [
    "# An Introduction to Reinforcement Learning Concepts and a Practical Guide\n",
    "\n",
    "In this guide, we will cover :\n",
    "1. What Reinforcement Learning is.\n",
    "2. Reinforcement Learning basic concepts and terminology.\n",
    "3. Applications of Reinforcement Learning.\n",
    "4. A practical guide to performing Reinforcement Learning with code. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf81573d",
   "metadata": {},
   "source": [
    "## 1. What is Reinforcement Learning?\n",
    "\n",
    "Most of you have heard of an AI that plays computer games and was actually able to defeat a world reknown South Korean player in the game. Well ... that is technically reinforcement learning at a glimpse. Let me show you how through a simple analogy.\n",
    "\n",
    "Imagine that you have a dog and that you're trying to teach it a new trick. In this situation, every time the dog's response is a desired one, you give it food and when the response is not desired, you don't! Hard I know! The idea is that the dog will learning from every positive experience it has had before, so it will keep making the desired response because it knows that it will be rewarded. It will also learn what not to do from the undesired responses. The dog is learning from it's past experiences which influence its present choices. This is reinforcement learning!\n",
    "\n",
    "### Definition of Reinforcement Learning\n",
    "\n",
    "It's a subset of Machine Learning which involves agents attempting to take actions in hopes of maximizing a prioritized reward. It is different from Supervised Learning (SL) whereby in SL ,the training data is labelled and therefore the model is trained with the correct answer, in Reinforcement Learning (RL), there is no answer, the agent decides what action to make and learns from its past experiences.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e227f1e",
   "metadata": {},
   "source": [
    "## 2. Basic Concepts and Terminology\n",
    "\n",
    "Here I will go over the main terminologies you need to understand:\n",
    "\n",
    "1. Agent - The one who makes decisions based on the rewards an punishments. In our analogy, the dog is the agent.\n",
    "2. Environment - The world in which the agent lives and interacts. In our analogy, this could be in your backyard or house.\n",
    "3. State - Current situation returned by the environment. An example of a state could be your dog standing and you using a specific word in a certain tone in your living room.\n",
    "4. Action - All the possible moves that the agent can make. A transition from one state to the other. In our analogy, this could be the dog going from standing to running to fetch the stick.\n",
    "5. Reward - An immediate return send back from the environment to evaluate the last action. In our analogy, the dog is given a treat when it does the desired treat and a penalty of \"No\" if it performs the undesired action.\n",
    "6. Policy - The strategy of choosing an action, given a state, in expectation of better outcomes.\n",
    "\n",
    "#### Putting it all together ...\n",
    "A Reinforcement Learning (RL) is about training an agent that interacts with its environment. The agent transitions between different scenarios of the environment, referred to as states, by performing actions. Actions, in return, yield rewards, which could be positive, negative or zero. The agentâ€™s sole purpose is to maximize the notion of cumulative reward over an episode, which is everything that happens between an initial state and a terminal state, where we decide the rewards which align with the tasks that we want to accomplish.\n",
    "Hence, we reinforce the agent to perform certain actions by providing it with positive rewards, and to stray away from others by providing negative rewards. This is how an agent learns to develop a strategy or policy.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33dee53",
   "metadata": {},
   "source": [
    "## 3. Applications of Reinforcement Learning\n",
    "\n",
    "1. It can be used in robotics for industrial automation.\n",
    "2. It can be used in machine learning and data processing.\n",
    "3. It can be used to create training systems that provide custom instruction and materials according to the requirement of students.\n",
    "4. Aircraft control and robot motion control.\n",
    "5. Business strategy planning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1232111b",
   "metadata": {},
   "source": [
    "## 4. A practical guide to performing Reinforcement Learning with code.\n",
    "\n",
    "### AI Learns to Play Atari Space Invaders using Deep Reinforcement Learning\n",
    "\n",
    "Deep Reinforcement Learning (DRL) is a subset of Machine Learning that combines neural networks with RL to help agents achieve their goals. We will use DRL techniques to taech our agent how to play the famous Atari Space Invaders game. If you want to learn more about the game beforehand, here you go: http://www.atarimania.com/game-atari-2600-vcs-space-invaders_s6947.html\n",
    "\n",
    "For the game, we need to create an environment where our agent can perform actions, get a score and get the current state. Fortunately, OpenAI Gym , already has this environment built for us. This library provide us with the API that provides our ageent with all the information it will need. We'll be using the gym environment 'SpaceInvaders-v0' for our game."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40293639",
   "metadata": {},
   "source": [
    "## Install Dependencies\n",
    "We use gym, keras-rl - a reinforcement learning library , atari-py which is a python binding to atari games,and tensorflow. Let's install them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10ef576e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym in c:\\users\\supreme computers\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (0.23.1)\n",
      "Requirement already satisfied: tensorflow in c:\\users\\supreme computers\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (2.5.0)\n",
      "Requirement already satisfied: atari_py in c:\\users\\supreme computers\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (0.2.9)\n",
      "Requirement already satisfied: keras-rl2 in c:\\users\\supreme computers\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (1.0.5)\n",
      "Requirement already satisfied: pygame in c:\\users\\supreme computers\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (2.1.2)\n",
      "Requirement already satisfied: importlib-metadata>=4.10.0 in c:\\users\\supreme computers\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from gym) (4.11.3)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\supreme computers\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from gym) (1.6.0)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in c:\\users\\supreme computers\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from gym) (0.0.6)\n",
      "Requirement already satisfied: numpy>=1.18.0 in c:\\users\\supreme computers\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from gym) (1.19.5)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\supreme computers\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from importlib-metadata>=4.10.0->gym) (3.8.0)\n",
      "Requirement already satisfied: six~=1.15.0 in c:\\users\\supreme computers\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied: absl-py~=0.10 in c:\\users\\supreme computers\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow) (0.13.0)\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in c:\\users\\supreme computers\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: h5py~=3.1.0 in c:\\users\\supreme computers\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow) (3.1.0)\n",
      "Requirement already satisfied: flatbuffers~=1.12.0 in c:\\users\\supreme computers\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow) (1.12)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in c:\\users\\supreme computers\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow) (3.17.3)\n",
      "Requirement already satisfied: tensorflow-estimator<2.6.0,>=2.5.0rc0 in c:\\users\\supreme computers\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow) (2.5.0)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in c:\\users\\supreme computers\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow) (1.12.1)\n",
      "Requirement already satisfied: opt-einsum~=3.3.0 in c:\\users\\supreme computers\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: google-pasta~=0.2 in c:\\users\\supreme computers\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: grpcio~=1.34.0 in c:\\users\\supreme computers\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow) (1.34.1)\n",
      "Requirement already satisfied: keras-nightly~=2.5.0.dev in c:\\users\\supreme computers\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow) (2.5.0.dev2021032900)\n",
      "Requirement already satisfied: wheel~=0.35 in c:\\users\\supreme computers\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow) (0.37.0)\n",
      "Requirement already satisfied: astunparse~=1.6.3 in c:\\users\\supreme computers\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: tensorboard~=2.5 in c:\\users\\supreme computers\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow) (2.5.0)\n",
      "Requirement already satisfied: termcolor~=1.1.0 in c:\\users\\supreme computers\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: typing-extensions~=3.7.4 in c:\\users\\supreme computers\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow) (3.7.4.3)\n",
      "Requirement already satisfied: gast==0.4.0 in c:\\users\\supreme computers\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\supreme computers\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorboard~=2.5->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\supreme computers\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorboard~=2.5->tensorflow) (3.3.4)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\supreme computers\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorboard~=2.5->tensorflow) (58.4.0)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in c:\\users\\supreme computers\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorboard~=2.5->tensorflow) (1.32.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\supreme computers\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorboard~=2.5->tensorflow) (1.8.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\users\\supreme computers\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorboard~=2.5->tensorflow) (2.0.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\supreme computers\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorboard~=2.5->tensorflow) (0.4.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\supreme computers\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorboard~=2.5->tensorflow) (2.25.1)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in c:\\users\\supreme computers\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow) (4.2.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\supreme computers\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\supreme computers\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow) (4.7.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\supreme computers\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\supreme computers\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\supreme computers\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\supreme computers\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\supreme computers\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (2021.5.30)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\supreme computers\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (1.26.6)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\supreme computers\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow) (3.1.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.1.1; however, version 22.0.4 is available.\n",
      "You should consider upgrading via the 'c:\\users\\supreme computers\\appdata\\local\\programs\\python\\python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "! pip install gym tensorflow atari_py keras-rl2 pygame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9575ac13",
   "metadata": {},
   "source": [
    "## Test a random environment using gym\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66deeec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\supreme computers\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\ale_py\\roms\\__init__.py:94: DeprecationWarning: Automatic importing of atari-py roms won't be supported in future releases of ale-py. Please migrate over to using `ale-import-roms` OR an ALE-supported ROM package. To make this warning disappear you can run `ale-import-roms --import-from-pkg atari_py.atari_roms`.For more information see: https://github.com/mgbellemare/Arcade-Learning-Environment#rom-management\n",
      "  _RESOLVED_ROMS = _resolve_roms()\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e0ec88",
   "metadata": {},
   "source": [
    "In the line of code below, we call the import rom method on our installed library, atari-py. We need a rom because it is a digital copy of our video game. You can make a quick google search or download it here: https://www.emulatorgames.net/space-invaders-roms/ . Load the file name at the specific file path that you saved on your local machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff0b0ccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Supreme Computers\\AppData\\Roaming\\Python\\Python39\\site-packages\\ipykernel\\ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copying space_invaders.bin from C:/Users/Supreme Computers/Documents/4.1/Knowledge-Based Systems/myroms/Space Invaders (1980) (Atari, Richard Maurer - Sears) (CX2632 - 49-75153) ~.bin to C:\\Users\\Supreme Computers\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\atari_py\\atari_roms\\space_invaders.bin\n"
     ]
    }
   ],
   "source": [
    "! python -m atari_py.import_roms \"C:/Users/Supreme Computers/Documents/4.1/Knowledge-Based Systems/myroms/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bddb2229",
   "metadata": {},
   "source": [
    "In the lines of code below, create the space invaders environment. Pull in the height, width and channels which are part of a state in the environment in order to pull it into a neural network. We can also call the get_action_meanings() function to see what they are: The agent can move left and right, fire left and right and noop means, no action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9db0d194",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('ALE/SpaceInvaders-v5',render_mode='human')\n",
    "height, width, channels = env.observation_space.shape\n",
    "actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae08c206",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NOOP', 'FIRE', 'RIGHT', 'LEFT', 'RIGHTFIRE', 'LEFTFIRE']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.unwrapped.get_action_meanings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633ce75a",
   "metadata": {},
   "source": [
    "Below, we test the environment on 5 episodes to see how it performs. We reset the environment for each episode. As the episode runs, the agent chooses a random action and is provided with information about the state and reward. The reward updated to the final ouput score. Here's how it performs without reinforcement Learning. \n",
    "\n",
    "You can notice that the scores are random and are not getting better with experience, let's now use RL to improve the agent's scores per game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6536db13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1 Score:120.0\n",
      "Episode:2 Score:40.0\n",
      "Episode:3 Score:110.0\n",
      "Episode:4 Score:95.0\n",
      "Episode:5 Score:125.0\n"
     ]
    }
   ],
   "source": [
    "episodes = 5\n",
    "for episode in range(1, episodes+1):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0 \n",
    "    \n",
    "    while not done:\n",
    "        action = random.choice([0,1,2,3,4,5])\n",
    "        n_state, reward, done, info = env.step(action)\n",
    "        score+=reward\n",
    "    print('Episode:{} Score:{}'.format(episode, score))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410ccf2c",
   "metadata": {},
   "source": [
    "## Create Deep Learning Model using Keras\n",
    "\n",
    "Import the suitable packages for this, listed below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6302105",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Convolution2D\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475e1e65",
   "metadata": {},
   "source": [
    "Start building the model and use the height, width, channels(pulled from observation space) and actions(pulled from the action space). The shape of these parameters will define our model.\n",
    "\n",
    "Sequential() is assigned to our model, so as to pass frames through sequentially.\n",
    "model.add() is used to stack layers in our network.\n",
    "\n",
    "The first layer is the Convolution2D layer with the following parameters:\n",
    "1. The number of filters (32). This is to train the model to detect objects in the frames such an enemy ships\n",
    "2. The size of the filters and the number of strides\n",
    "3. The ReLu function and the shape of the frame\n",
    "\n",
    "The second and thirs layer are all the same , just with different parameter\n",
    "Then flatten all the layers into a single layer, to pass it through a dense (fully-connected) layer. The first dense layer has 512 units. The second dense layer compresses this slightly and is 256 units. The third dense layer has the number of actions, which is 6 units.\n",
    "Finally, build the model by assigning the build_model function to model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "741aeacb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Supreme Computers\\AppData\\Roaming\\Python\\Python39\\site-packages\\ipykernel\\ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "def build_model(height, width, channels, actions):\n",
    "    model = Sequential()\n",
    "    model.add(Convolution2D(32, (8,8), strides=(4,4), activation='relu', input_shape=(3,height, width, channels)))\n",
    "    model.add(Convolution2D(64, (4,4), strides=(2,2), activation='relu'))\n",
    "    model.add(Convolution2D(64, (3,3), activation='relu'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dense(actions, activation='linear'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eb3e0766",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "01ed15cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(height, width, channels, actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "20052c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_3 (Conv2D)            (None, 3, 51, 39, 32)     6176      \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 3, 24, 18, 64)     32832     \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 3, 22, 16, 64)     36928     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 67584)             0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 512)               34603520  \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 6)                 1542      \n",
      "=================================================================\n",
      "Total params: 34,812,326\n",
      "Trainable params: 34,812,326\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0198c42c",
   "metadata": {},
   "source": [
    "## Build the RL Agent\n",
    "\n",
    "Finally, Build the RL agent with Keras. Import  SequentialMemory, which allows the agent to retain memory from previous games. Also import LinearAnnealedPolicy, which adds a decay as we get closer to the optimal strategy, and EpsGreedyPolicy, which allows the agent to find the best reward outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "634ec3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.agents import DQNAgent\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.policy import LinearAnnealedPolicy, EpsGreedyQPolicy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ae70ea",
   "metadata": {},
   "source": [
    "Define the agent function and pass through the model and actions. Then define the policy by passing the epsilon greedy policy through the linear anneal policy. Define the memory using sequential memory with a buffer limit of 1000 episodes with a window length of (meaning for 1000 episodes, the model stores the past 3 windows to capture what our previous steps look like).\n",
    "Then define the agent by passing through the model, memory, and policy. Add a dueling network, which splits the value and advantage and helps the model learn when to take actions and when not to take actions. Add which actions to take and the number of steps the model should take."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3be93478",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_agent(model, actions):\n",
    "    policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr='eps', value_max=1., value_min=.1, value_test=.2, nb_steps=10000)\n",
    "    memory = SequentialMemory(limit=1000, window_length=3)\n",
    "    dqn = DQNAgent(model=model, memory=memory, policy=policy,\n",
    "                  enable_dueling_network=True, dueling_type='avg', \n",
    "                   nb_actions=actions, nb_steps_warmup=1000\n",
    "                  )\n",
    "    return dqn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7cebd4d",
   "metadata": {},
   "source": [
    "Assign the build_agent function to dqn and add an optimizer for the neural network at a learning rate of .0001. Fit the model and training it 1000 steps. For reference, Google DeepMind suggests that 10 to 40 Million steps would be the best amoount of steps for optimal RL but for purposes of learning, I train my model for 10,000 steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "76e54597",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn = build_agent(model, actions)\n",
    "dqn.compile(Adam(learning_rate=1e-4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c476a60f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 1000 steps ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\supreme computers\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:2426: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 330/1000: episode: 1, duration: 73.034s, episode steps: 330, steps per second:   5, episode reward: 50.000, mean reward:  0.152 [ 0.000, 30.000], mean action: 2.436 [0.000, 5.000],  loss: --, mean_q: --, mean_eps: --\n",
      " 845/1000: episode: 2, duration: 80.824s, episode steps: 515, steps per second:   6, episode reward: 180.000, mean reward:  0.350 [ 0.000, 30.000], mean action: 2.643 [0.000, 5.000],  loss: --, mean_q: --, mean_eps: --\n",
      "done, took 178.273 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x19751ab92b0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn.fit(env, nb_steps=1000, visualize=False, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd137ba",
   "metadata": {},
   "source": [
    "## Test Model\n",
    "\n",
    "Let's test the model for 10 episodes and ouput the reward. From the scores, you can see that the agent had started learning from the training process. If the model was trained for 40M steps, it would have arrived at higher rewards and an optimal strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b507c392",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Supreme Computers\\AppData\\Roaming\\Python\\Python39\\site-packages\\ipykernel\\ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 10 episodes ...\n",
      "Episode 1: reward: 30.000, steps: 377\n",
      "Episode 2: reward: 70.000, steps: 467\n",
      "Episode 3: reward: 65.000, steps: 514\n",
      "Episode 4: reward: 140.000, steps: 723\n",
      "Episode 5: reward: 45.000, steps: 497\n",
      "Episode 6: reward: 160.000, steps: 477\n",
      "Episode 7: reward: 140.000, steps: 706\n",
      "Episode 8: reward: 215.000, steps: 792\n",
      "Episode 9: reward: 465.000, steps: 818\n",
      "Episode 10: reward: 225.000, steps: 684\n",
      "155.5\n"
     ]
    }
   ],
   "source": [
    "scores = dqn.test(env, nb_episodes=10, visualize=False)\n",
    "print(np.mean(scores.history['episode_reward']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a9f1e6",
   "metadata": {},
   "source": [
    "## The end\n",
    "I hope you learned an interesting concept from today's practical guide."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
